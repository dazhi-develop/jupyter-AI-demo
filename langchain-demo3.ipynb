{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"最后一个问题.txt\",encoding = 'UTF-8')\n",
    "data = loader.load()\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# loader = CSVLoader(file_path='./mlb_teams_2012.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"最后一个问题.txt\",encoding = 'UTF-8') as f:\n",
    "    last_question = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([last_question])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new =[]\n",
    "for text in texts:\n",
    "    texts_new.append(text.page_content)\n",
    "\n",
    "\n",
    "print(type(texts_new)) \n",
    "print(texts_new)\n",
    "\n",
    "\n",
    "embeddings_new = embeddings_model.embed_documents(\n",
    "    texts_new\n",
    ")\n",
    "len(embeddings_new), len(embeddings_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"这最后一个问题第一次提出来?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chromadb\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, OpenAIEmbeddings())\n",
    "query = \"于是就有了光\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人说：“你何时能获得足够资料回答这个问题?” \n",
      "\n",
      "太宇埃克说：“目前还无足够资料作出有效回答。” \n",
      "\n",
      "“你会坚持不懈地寻求答案吗?”人说。 \n",
      "\n",
      "“是的。”太宇埃克回答。 \n",
      "\n",
      "\n",
      "群星熄灭了，各银河系也相继咽了气，太空经过几十亿年的衰老，愈趋黑暗。 \n",
      "\n",
      "人一个接一个地与埃克溶合，每一物质个体以这种在某种程度上不是失去而是获得的方式放弃了它的精神个性。 \n",
      "\n",
      "人的最后精神在溶进前停顿了一下，巡视这个太空。太空中空空如也，仅剩下最末一个黑暗恒星的残渣，其中也只剩下难以想象得稀薄的物质被行将燃尽的余热胡乱搅动着，逐渐趋向绝对零度。 \n",
      "\n",
      "人说：“埃克!这就是末日吗?这无底深渊不能再倒转成宇宙吗?” \n",
      "\n",
      "埃克说：“目前还无足够资料作出有效回答。” \n",
      "\n",
      "人的最后精神溶进去了，于是只剩埃克存在——存在于超空间中。 \n",
      "\n",
      "物质和能量消灭了，空间和时间也随之结束。甚至埃克也只是为了那最后一个问题而存在。自从十万亿年前一个半醉的电脑技术员提出这个问题以来，它始终没有得到解答。 \n",
      "\n",
      "其他一切问题都得到了解答，除非这最后一个问题也得到解答，埃克大约是不会放弃他的意识的。 \n",
      "\n",
      "一切资料都汇集完了，再没有资料可汇集了。埃克终于懂得了怎样倒转熵的方向。 \n",
      "\n",
      "然而，再也没有一个人，埃克可以向他回答这个最后的问题了。没关系，答案本身也能使工作进行下去。 \n",
      "\n",
      "在另一个超时间间隔中，埃克想好了怎样才能把活干得最漂亮。他小心翼翼地编制起整套程序。 \n",
      "\n",
      "埃克的意识曾一度包孕住宇宙的一切，结论将展现在的无底深渊上面。一步一步地，工作即将完成。 \n",
      "\n",
      "接着，埃克说：“要有光!” \n",
      "\n",
      "于是就有了光——\n"
     ]
    }
   ],
   "source": [
    "# Vector store-backed retriever\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"于是就有了光\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人说：“你何时能获得足够资料回答这个问题?” \n",
      "\n",
      "太宇埃克说：“目前还无足够资料作出有效回答。” \n",
      "\n",
      "“你会坚持不懈地寻求答案吗?”人说。 \n",
      "\n",
      "“是的。”太宇埃克回答。 \n",
      "\n",
      "\n",
      "群星熄灭了，各银河系也相继咽了气，太空经过几十亿年的衰老，愈趋黑暗。 \n",
      "\n",
      "人一个接一个地与埃克溶合，每一物质个体以这种在某种程度上不是失去而是获得的方式放弃了它的精神个性。 \n",
      "\n",
      "人的最后精神在溶进前停顿了一下，巡视这个太空。太空中空空如也，仅剩下最末一个黑暗恒星的残渣，其中也只剩下难以想象得稀薄的物质被行将燃尽的余热胡乱搅动着，逐渐趋向绝对零度。 \n",
      "\n",
      "人说：“埃克!这就是末日吗?这无底深渊不能再倒转成宇宙吗?” \n",
      "\n",
      "埃克说：“目前还无足够资料作出有效回答。” \n",
      "\n",
      "人的最后精神溶进去了，于是只剩埃克存在——存在于超空间中。 \n",
      "\n",
      "物质和能量消灭了，空间和时间也随之结束。甚至埃克也只是为了那最后一个问题而存在。自从十万亿年前一个半醉的电脑技术员提出这个问题以来，它始终没有得到解答。 \n",
      "\n",
      "其他一切问题都得到了解答，除非这最后一个问题也得到解答，埃克大约是不会放弃他的意识的。 \n",
      "\n",
      "一切资料都汇集完了，再没有资料可汇集了。埃克终于懂得了怎样倒转熵的方向。 \n",
      "\n",
      "然而，再也没有一个人，埃克可以向他回答这个最后的问题了。没关系，答案本身也能使工作进行下去。 \n",
      "\n",
      "在另一个超时间间隔中，埃克想好了怎样才能把活干得最漂亮。他小心翼翼地编制起整套程序。 \n",
      "\n",
      "埃克的意识曾一度包孕住宇宙的一切，结论将展现在的无底深渊上面。一步一步地，工作即将完成。 \n",
      "\n",
      "接着，埃克说：“要有光!” \n",
      "\n",
      "于是就有了光——\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.get_relevant_documents(\"于是就有了光\")\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?utm_source=yt&utm_medium=social&utm_campaign=gemini24#build-experiment\")\n",
    "data2 = loader.load()\n",
    "\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter2.split_documents(data2)\n",
    "print(len(splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "question = \"does gemini can perform understanding and reasoning tasks for different modalities\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can Gemini effectively handle understanding and reasoning tasks across various modalities?', '2. Is Gemini capable of performing tasks that require comprehension and logical reasoning across different types of data?', '3. Does Gemini have the ability to process and reason through information from multiple modalities effectively?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='10:25\\n\\n\\n\\n\\nGemini 1.5 Pro can understand, reason about and identify curious details in the 402-page transcripts from Apollo 11’s mission to the moon.' metadata={'description': 'Gemini 1.5 delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.', 'language': 'en-us', 'source': 'https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?utm_source=yt&utm_medium=social&utm_campaign=gemini24#build-experiment', 'title': \"Introducing Gemini 1.5, Google's next-generation AI model\"}\n"
     ]
    }
   ],
   "source": [
    "print(unique_docs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
